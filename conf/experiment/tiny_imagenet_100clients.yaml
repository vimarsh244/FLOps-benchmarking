# @package _global_
# Tiny ImageNet experiment with 100 clients
# Uses ResNet18 with FedProx (good for heterogeneous data)

defaults:
  - override /dataset: tiny_imagenet
  - override /model: resnet18
  - override /strategy: fedprox
  - override /partitioner: dirichlet
  - override /scenario: baseline
  - override /logging: wandb
  - override /scenario: node_drop_extreme


# experiment identification
experiment:
  name: tiny_imagenet_100clients_fedprox_niid_0.5_node_drop_extreme
  seed: 42

# server configuration for large scale
server:
  num_rounds: 100
  fraction_fit: 0.9       # sample 10% of clients per round (10 clients)
  fraction_evaluate: 1.0
  min_fit_clients: 10
  min_evaluate_clients: 10
  min_available_clients: 100
  accept_failures: true

# client configuration
client:
  num_clients: 100
  local_epochs: 3         # more local epochs for larger dataset
  batch_size: 64
  learning_rate: 0.001    # lower lr for larger model
  optimizer: adam

# strategy config (fedprox with moderate regularization)
strategy:
  proximal_mu: 0.01       # proximal term to handle heterogeneity

# partitioner config (non-iid for realistic scenario)
partitioner:
  alpha: 0.5              # moderate heterogeneity

# training configuration
training:
  device: auto
  use_amp: false

# evaluation
evaluation:
  test_fraction: 0.2
  eval_every_n_rounds: 1

# hardware config for 100 clients
hardware:
  ray:
    num_cpus: 8
    num_gpus: 2
    client_resources:
      num_cpus: 2
      num_gpus: 0.5       # share GPU across 10 clients

